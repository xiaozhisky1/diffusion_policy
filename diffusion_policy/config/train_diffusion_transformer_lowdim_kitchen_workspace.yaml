# 默认配置项
defaults:
  - _self_  # 使用当前配置文件的自身设置
  - task: kitchen_lowdim_abs  # 设置任务名称为 "kitchen_lowdim_abs"。这里是一个在厨房环境下的低维任务

# 训练配置名称
name: train_diffusion_transformer_lowdim  # 配置名称为 "train_diffusion_transformer_lowdim"

# 定义目标训练工作空间，训练过程中使用的具体方法
_target_: diffusion_policy.workspace.train_diffusion_transformer_lowdim_workspace.TrainDiffusionTransformerLowdimWorkspace

# 任务相关的维度和名称，连接到当前任务的相关参数
obs_dim: ${task.obs_dim}  # 观察空间的维度，由任务定义
action_dim: ${task.action_dim}  # 动作空间的维度，由任务定义
task_name: ${task.name}  # 任务的名称
exp_name: "default"  # 实验名称，默认为 "default"

# 训练过程的参数
horizon: 16  # 每个训练 episode 的最大时间步数。这个值决定了智能体每次与环境交互的最大时间步数。
n_obs_steps: 4  # 在每个训练步骤中，使用多少个观察数据作为输入来预测动作
n_action_steps: 8  # 在每个训练步骤中，智能体实际执行的动作步数
n_latency_steps: 0  # 延迟步数，表示智能体等待多少步后执行动作，通常用于引入延迟响应
past_action_visible: False  # 是否让智能体看到过去的动作。设置为 False 时，智能体只能看到当前的观察
keypoint_visible_rate: 1.0  # 关键点的可见率，控制关键点数据在训练过程中的使用比例
obs_as_cond: True  # 是否将观察作为条件传入模型，如果设置为 True，则模型会将观察作为条件输入来进行预测
pred_action_steps_only: False  # 是否只预测动作步数，设置为 False 时模型会预测整个过程的动作

# 定义策略模型的配置
policy:
  _target_: diffusion_policy.policy.diffusion_transformer_lowdim_policy.DiffusionTransformerLowdimPolicy  # 使用的具体策略模型类

  # 策略模型的具体参数
  model:
    _target_: diffusion_policy.model.diffusion.transformer_for_diffusion.TransformerForDiffusion  # 使用的模型是一个基于 Transformer 的扩散模型
    input_dim: ${eval:'${action_dim} if ${obs_as_cond} else ${obs_dim} + ${action_dim}'}  # 输入维度的设置，若 `obs_as_cond` 为 True，输入维度为动作空间维度，否则为观察空间维度与动作空间维度的和
    output_dim: ${policy.model.input_dim}  # 输出维度与输入维度一致
    horizon: ${horizon}  # horizon 参数，通常指每个 episode 的最大时间步数
    n_obs_steps: ${n_obs_steps}  # 每次预测时使用的观察步数
    cond_dim: ${eval:'${obs_dim} if ${obs_as_cond} else 0'}  # 如果 `obs_as_cond` 为 True，则条件维度是观察维度，否则为 0

    # Transformer 模型的超参数设置
    n_layer: 8  # Transformer 中的层数
    n_head: 4  # Transformer 中的头数（Multi-Head Attention）
    n_emb: 768  # 嵌入维度，表示每个 token 的表示维度
    p_drop_emb: 0.0  # 嵌入层的 dropout 概率
    p_drop_attn: 0.1  # 注意力层的 dropout 概率

    # 其他 Transformer 相关设置
    causal_attn: True  # 是否使用因果注意力机制（Causal Attention），即模型只关注过去的信息，不使用未来的信息
    time_as_cond: True  # 是否将时间作为条件输入到模型中。如果为 False，则只使用类似 BERT 的编码器结构
    obs_as_cond: ${obs_as_cond}  # 是否使用观察作为条件输入
    n_cond_layers: 0  # 条件层数，若大于 0，则在模型中使用 Transformer 编码器来处理条件输入，否则使用 MLP（多层感知机）

  # 定义噪声调度器，用于控制训练过程中噪声的添加过程
  noise_scheduler:
    _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler  # 使用 DDPM (Denoising Diffusion Probabilistic Model) 的噪声调度器
    num_train_timesteps: 100  # 训练过程中总的时间步数（即迭代次数）
    beta_start: 0.0001  # 初始噪声系数
    beta_end: 0.02  # 最终噪声系数
    beta_schedule: squaredcos_cap_v2  # 噪声系数调度的方式，这里使用平方余弦调度
    variance_type: fixed_small  # 噪声方差类型，表示噪声的方差是固定的小值
    clip_sample: True  # 是否在采样时进行裁剪，防止出现无效的样本
    prediction_type: epsilon  # 预测类型，选择是否预测噪声（epsilon）或者样本（sample）


  # 训练过程的核心超参数
  horizon: ${horizon}  # 训练过程中每个episode的最大时间步数，即智能体与环境交互的最大步数
  obs_dim: ${obs_dim}  # 观察空间的维度，表示智能体从环境中接收到的状态信息的维度
  action_dim: ${action_dim}  # 动作空间的维度，表示智能体可执行动作的维度
  n_action_steps: ${n_action_steps}  # 动作的步数，定义每个训练步骤中智能体需要执行的动作步数
  n_obs_steps: ${n_obs_steps}  # 观察的步数，定义每个训练步骤中智能体用于预测的观察数量
  num_inference_steps: 100  # 推理步骤数，通常表示每个模型在训练中进行的推理步骤数
  obs_as_cond: ${obs_as_cond}  # 是否将观察作为条件输入到模型中，如果是，则模型会根据观察生成动作
  pred_action_steps_only: ${pred_action_steps_only}  # 是否仅预测动作步数，若为True，则模型只预测动作，不考虑完整的状态序列

# EMA模型设置（Exponential Moving Average，指数加权平均）
ema:
  _target_: diffusion_policy.model.diffusion.ema_model.EMAModel  # 使用的EMA模型
  update_after_step: 0  # EMA更新的步数间隔，这里是每0步更新一次
  inv_gamma: 1.0  # EMA更新公式中的倒数γ，用于控制权重的衰减
  power: 0.75  # EMA更新时的指数幂指数
  min_value: 0.0  # EMA值的最小限制
  max_value: 0.9999  # EMA值的最大限制

# 数据加载器配置
dataloader:
  batch_size: 256  # 每个批次的大小，决定每次训练时处理多少个样本
  num_workers: 1  # 数据加载器使用的并行工作进程数，1表示单线程加载
  shuffle: True  # 是否对数据进行洗牌（随机化）
  pin_memory: True  # 是否将数据加载到固定内存中以加快GPU加载速度
  persistent_workers: False  # 是否使用持久化的工作进程，避免每次加载都重新启动进程

# 验证集的数据加载器配置
val_dataloader:
  batch_size: 256  # 验证集批次大小
  num_workers: 1  # 验证集数据加载的工作进程数
  shuffle: False  # 验证集不需要洗牌
  pin_memory: True  # 验证集数据加载到GPU时启用内存固定
  persistent_workers: False  # 验证集使用非持久化工作进程

# 优化器的配置
optimizer:
  learning_rate: 1.0e-4  # 学习率，控制每次参数更新的步伐
  weight_decay: 1.0e-3  # 权重衰减（L2正则化），防止过拟合
  betas: [0.9, 0.95]  # Adam优化器的动量参数，控制一阶矩估计和二阶矩估计的衰减

# 训练过程的配置
training:
  device: "cuda:0"  # 设置训练使用的设备，这里使用第0块GPU
  seed: 42  # 设置随机种子，保证实验结果的可复现性
  debug: False  # 是否开启调试模式，设置为True时会打印更多的调试信息
  resume: True  # 是否从上次训练的检查点恢复训练，True表示恢复，False表示重新训练
  # 优化设置
  lr_scheduler: cosine  # 学习率调度策略，这里使用余弦退火调度器（cosine annealing）
  lr_warmup_steps: 1000  # 学习率预热的步数，帮助优化器在训练初期逐渐适应
  num_epochs: 5000  # 训练的总epoch数，即训练的轮数
  gradient_accumulate_every: 1  # 每多少步进行一次梯度累积，1表示每步都更新
  use_ema: True  # 是否使用EMA（指数移动平均）平滑模型参数，通常在训练中使用以提高泛化能力
  # 控制训练过程的检查点、验证等间隔
  rollout_every: 50  # 每50个epoch进行一次策略滚动（计算新策略）
  checkpoint_every: 50  # 每50个epoch保存一次检查点
  val_every: 1  # 每1个epoch进行一次验证
  sample_every: 5  # 每5个epoch采样一次样本，用于日志记录和评估
  # 每个epoch中的最大训练步数和验证步数
  max_train_steps: null  # 每个epoch最大训练步数，设置为null表示没有限制
  max_val_steps: null  # 每个验证集epoch最大验证步数，设置为null表示没有限制
  # 其他设置
  tqdm_interval_sec: 1.0  # 显示进度条更新的时间间隔

# 日志记录配置
logging:
  project: diffusion_policy_debug  # 日志记录项目名称
  resume: True  # 是否从上次日志恢复
  mode: online  # 日志记录模式，online表示实时记录
  name: ${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}  # 日志文件的名称，包含当前时间、任务名称等
  tags: ["${name}", "${task_name}", "${exp_name}"]  # 日志的标签，用于区分不同实验
  id: null  # 日志记录的ID（可选）
  group: null  # 日志记录的组（可选）

# 检查点保存配置
checkpoint:
  topk:
    monitor_key: test_mean_score  # 监控的指标是验证集上的平均测试得分
    mode: max  # 模式是最大化，即保存最高得分的模型
    k: 5  # 保存得分前5的模型
    format_str: 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt'  # 保存检查点的格式
  save_last_ckpt: True  # 是否保存最后的检查点
  save_last_snapshot: False  # 是否保存最后的快照

# 多次实验的设置
multi_run:
  run_dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}  # 设置实验结果保存路径，包括时间戳
  wandb_name_base: ${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}  # 使用wandb时，实验名称的基本格式

# Hydra的配置
hydra:
  job:
    override_dirname: ${name}  # 设置job的目录名，使用实验名称
  run:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}  # 设置运行结果保存路径
  sweep:
    dir: data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}  # 设置超参数搜索的输出路径
    subdir: ${hydra.job.num}  # 使用不同的子目录保存不同的实验配置

